{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f25ef07-9f97-4351-acd2-85cde9964e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03.png to RGBA.\n",
      "Converted 05.png to RGBA.\n",
      "Converted 02.png to RGBA.\n",
      "Converted 01.png to RGBA.\n",
      "Converted 08.png to RGBA.\n",
      "Converted 10.png to RGBA.\n",
      "Converted 07.png to RGBA.\n",
      "Converted 04.png to RGBA.\n",
      "Converted 06.png to RGBA.\n",
      "Converted 09.png to RGBA.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "input_folder = \"./data\"\n",
    "output_folder = \"./data\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".png\"):  \n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Read the image (forcefully add an alpha channel if missing)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if img.shape[2] == 3:  # If no alpha channel, add one\n",
    "            b, g, r = cv2.split(img)\n",
    "            a = np.ones_like(b) * 255  # Create a full white alpha channel\n",
    "            img = cv2.merge((b, g, r, a))\n",
    "\n",
    "        # Save image\n",
    "        cv2.imwrite(os.path.join(output_folder, filename), img)\n",
    "        print(f\"Converted {filename} to RGBA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f27b68d-95eb-4724-8d5b-abb510a9a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553bd1d9-474b-4f56-95c7-5822ef3f809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_files(path):\n",
    "    '''\n",
    "    Creates a list with the names of the images to be processed.\n",
    "    '''\n",
    "    image_files = []\n",
    "    # creates a ScandirIterator aliased as files\n",
    "    with os.scandir(path) as files:\n",
    "        for file in files:\n",
    "            if file.name.endswith('.jpg'):\n",
    "                # adds only the image files to the list\n",
    "                image_files.append(file.name)\n",
    "    return sorted(image_files)\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    model = VGG16()\n",
    "    model = Model(inputs = model.inputs, outputs = model.output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def recognize_objects(file, path, model):\n",
    "    '''\n",
    "    Recognizes objects in an image file using the VGG16 model.\n",
    "    '''\n",
    "    # load the image as a 224x224 array\n",
    "    img = load_img(f'{path}/{file}', target_size=(224,224))\n",
    "    # convert from 'PIL.Image.Image' to numpy array\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3)\n",
    "    # prepare image for model\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    imgx = tf.convert_to_tensor(imgx)\n",
    "    # get object names\n",
    "    objects = model.predict(imgx)\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6885f8-8cdf-47e5-a2f9-384cc656f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rallypal/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor_92']\n",
      "Received: inputs=Tensor(shape=(1, 224, 224, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects detected in 01.jpg are:\n",
      "1. crutch (7.83%)\n",
      "2. swab (7.47%)\n",
      "3. drum (6.01%)\n",
      "4. panpipe (5.55%)\n",
      "5. sombrero (3.34%)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Objects detected in 02.jpg are:\n",
      "1. fountain (15.15%)\n",
      "2. swab (6.54%)\n",
      "3. crutch (6.14%)\n",
      "4. umbrella (5.21%)\n",
      "5. trench_coat (4.38%)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Objects detected in 03.jpg are:\n",
      "1. trench_coat (9.80%)\n",
      "2. hair_spray (9.43%)\n",
      "3. abaya (8.95%)\n",
      "4. academic_gown (7.39%)\n",
      "5. cleaver (7.20%)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "Objects detected in 04.jpg are:\n",
      "1. library (22.30%)\n",
      "2. gondola (19.80%)\n",
      "3. bookshop (18.81%)\n",
      "4. streetcar (16.26%)\n",
      "5. prison (2.46%)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "Objects detected in 05.jpg are:\n",
      "1. obelisk (16.47%)\n",
      "2. church (9.61%)\n",
      "3. beacon (9.59%)\n",
      "4. flagpole (8.93%)\n",
      "5. megalith (7.47%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"./data\"\n",
    "model = load_model()\n",
    "#print(model.input_shape)  # Expected input shape\n",
    "\n",
    "files = image_files(path)\n",
    "\n",
    "for file in files: \n",
    "    preds= recognize_objects(file, path, model)\n",
    "    top_preds = decode_predictions(preds, top=5)[0]\n",
    "\n",
    "    print(f\"Objects detected in {file} are:\")\n",
    "    for i, (imagenet_id, label, score) in enumerate(top_preds):\n",
    "        print(f\"{i+1}. {label} ({score*100:.2f}%)\")\n",
    "    print()\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
